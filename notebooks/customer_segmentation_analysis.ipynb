{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tusker Loyalty Customer Segmentation Analysis\n",
    "## Achieving Silhouette Score >0.85 for Customer Clustering\n",
    "\n",
    "This notebook demonstrates customer segmentation using unsupervised learning techniques with a focus on achieving high clustering performance (Silhouette score >0.85) as specified in the PRD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_df = pd.read_csv('../data/train_data.csv')\n",
    "test_df = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nTraining data columns: {list(train_df.columns)}\")\n",
    "\n",
    "# Display basic info\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "def create_eda_visualizations(df):\n",
    "    \"\"\"Create comprehensive EDA visualizations\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Tusker Loyalty Customer Data - Exploratory Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Age distribution\n",
    "    axes[0,0].hist(df['age'], bins=30, alpha=0.7, color='#DAA520')\n",
    "    axes[0,0].set_title('Age Distribution')\n",
    "    axes[0,0].set_xlabel('Age')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Income distribution\n",
    "    income_counts = df['income_kes'].value_counts()\n",
    "    axes[0,1].pie(income_counts.values, labels=income_counts.index, autopct='%1.1f%%')\n",
    "    axes[0,1].set_title('Income Distribution')\n",
    "    \n",
    "    # Purchase history vs engagement\n",
    "    axes[0,2].scatter(df['purchase_history'], df['engagement_rate'], alpha=0.6, color='#228B22')\n",
    "    axes[0,2].set_title('Purchase History vs Engagement Rate')\n",
    "    axes[0,2].set_xlabel('Purchase History')\n",
    "    axes[0,2].set_ylabel('Engagement Rate')\n",
    "    \n",
    "    # County distribution (top 10)\n",
    "    top_counties = df['county'].value_counts().head(10)\n",
    "    axes[1,0].barh(range(len(top_counties)), top_counties.values)\n",
    "    axes[1,0].set_yticks(range(len(top_counties)))\n",
    "    axes[1,0].set_yticklabels(top_counties.index)\n",
    "    axes[1,0].set_title('Top 10 Counties')\n",
    "    axes[1,0].set_xlabel('Customer Count')\n",
    "    \n",
    "    # Upgrade engagement distribution\n",
    "    axes[1,1].hist(df['upgrade_engagement'], bins=30, alpha=0.7, color='#00BFFF')\n",
    "    axes[1,1].set_title('M-Pesa Upgrade Engagement (2025)')\n",
    "    axes[1,1].set_xlabel('Upgrade Engagement Score')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Segment target distribution\n",
    "    segment_counts = df['segment_target'].value_counts()\n",
    "    axes[1,2].bar(range(len(segment_counts)), segment_counts.values)\n",
    "    axes[1,2].set_xticks(range(len(segment_counts)))\n",
    "    axes[1,2].set_xticklabels(segment_counts.index, rotation=45, ha='right')\n",
    "    axes[1,2].set_title('Target Segments')\n",
    "    axes[1,2].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_eda_visualizations(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering and Preprocessing\n",
    "def preprocess_features(df):\n",
    "    \"\"\"Preprocess features for clustering\"\"\"\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_income = LabelEncoder()\n",
    "    le_behavior = LabelEncoder()\n",
    "    le_county = LabelEncoder()\n",
    "    le_profit = LabelEncoder()\n",
    "    \n",
    "    df_processed['income_encoded'] = le_income.fit_transform(df_processed['income_kes'])\n",
    "    df_processed['behavior_encoded'] = le_behavior.fit_transform(df_processed['behavior'])\n",
    "    df_processed['county_encoded'] = le_county.fit_transform(df_processed['county'])\n",
    "    df_processed['profit_encoded'] = le_profit.fit_transform(df_processed['profit_segment'])\n",
    "    \n",
    "    # Select features for clustering\n",
    "    feature_cols = [\n",
    "        'age', 'income_encoded', 'purchase_history', 'behavior_encoded',\n",
    "        'engagement_rate', 'county_encoded', 'profit_encoded', 'upgrade_engagement'\n",
    "    ]\n",
    "    \n",
    "    X = df_processed[feature_cols]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, feature_cols, (le_income, le_behavior, le_county, le_profit, scaler)\n",
    "\n",
    "# Preprocess training data\n",
    "X_train_scaled, feature_cols, encoders = preprocess_features(train_df)\n",
    "print(f\"Processed features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Feature columns: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize K-means clustering for Silhouette score >0.85\n",
    "def find_optimal_clusters(X, max_k=15):\n",
    "    \"\"\"Find optimal number of clusters using multiple metrics\"\"\"\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    inertias = []\n",
    "    k_range = range(2, max_k + 1)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        \n",
    "        print(f\"k={k}: Silhouette Score = {silhouette_avg:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Silhouette scores\n",
    "    ax1.plot(k_range, silhouette_scores, 'bo-', linewidth=2, markersize=8)\n",
    "    ax1.axhline(y=0.85, color='r', linestyle='--', label='Target Score (0.85)')\n",
    "    ax1.set_xlabel('Number of Clusters (k)')\n",
    "    ax1.set_ylabel('Silhouette Score')\n",
    "    ax1.set_title('Silhouette Score vs Number of Clusters')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Elbow method\n",
    "    ax2.plot(k_range, inertias, 'ro-', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Number of Clusters (k)')\n",
    "    ax2.set_ylabel('Inertia')\n",
    "    ax2.set_title('Elbow Method')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best k for target silhouette score\n",
    "    best_scores = [(k, score) for k, score in zip(k_range, silhouette_scores) if score > 0.85]\n",
    "    \n",
    "    if best_scores:\n",
    "        best_k = min(best_scores, key=lambda x: x[0])[0]  # Choose smallest k that meets criteria\n",
    "        print(f\"\\n✅ Found k={best_k} with Silhouette score > 0.85\")\n",
    "    else:\n",
    "        best_k = k_range[np.argmax(silhouette_scores)]\n",
    "        print(f\"\\n⚠️ Best k={best_k} with Silhouette score = {max(silhouette_scores):.4f} (below 0.85)\")\n",
    "    \n",
    "    return best_k, silhouette_scores\n",
    "\n",
    "# Find optimal clusters\n",
    "optimal_k, scores = find_optimal_clusters(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced clustering with PCA for better performance\n",
    "def optimize_clustering_with_pca(X, target_score=0.85):\n",
    "    \"\"\"Use PCA to improve clustering performance\"\"\"\n",
    "    \n",
    "    # Try different PCA components\n",
    "    best_score = 0\n",
    "    best_config = None\n",
    "    \n",
    "    for n_components in [3, 4, 5, 6]:\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        print(f\"\\nPCA with {n_components} components (explained variance: {pca.explained_variance_ratio_.sum():.3f})\")\n",
    "        \n",
    "        for k in range(2, 12):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "            labels = kmeans.fit_predict(X_pca)\n",
    "            score = silhouette_score(X_pca, labels)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_config = (n_components, k, pca, kmeans, X_pca)\n",
    "            \n",
    "            if score > target_score:\n",
    "                print(f\"  k={k}: Silhouette = {score:.4f} ✅\")\n",
    "            else:\n",
    "                print(f\"  k={k}: Silhouette = {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Best configuration: PCA({best_config[0]}) + K-means({best_config[1]}) = {best_score:.4f}\")\n",
    "    \n",
    "    return best_config\n",
    "\n",
    "# Optimize with PCA\n",
    "best_config = optimize_clustering_with_pca(X_train_scaled)\n",
    "n_components, best_k, pca, best_kmeans, X_pca = best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model training and evaluation\n",
    "final_labels = best_kmeans.predict(X_pca)\n",
    "final_score = silhouette_score(X_pca, final_labels)\n",
    "\n",
    "print(f\"🏆 FINAL MODEL PERFORMANCE\")\n",
    "print(f\"Silhouette Score: {final_score:.4f}\")\n",
    "print(f\"Target Achievement: {'✅ PASSED' if final_score > 0.85 else '❌ NEEDS IMPROVEMENT'}\")\n",
    "print(f\"Number of Clusters: {best_k}\")\n",
    "print(f\"PCA Components: {n_components}\")\n",
    "\n",
    "# Add cluster labels to training data\n",
    "train_df_clustered = train_df.copy()\n",
    "train_df_clustered['predicted_cluster'] = final_labels\n",
    "\n",
    "# Cluster analysis\n",
    "print(f\"\\n📊 CLUSTER DISTRIBUTION:\")\n",
    "cluster_counts = pd.Series(final_labels).value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count:,} customers ({count/len(final_labels):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "def visualize_clusters(X_pca, labels, title=\"Customer Segments - PCA Visualization\"):\n",
    "    \"\"\"Create 2D and 3D cluster visualizations\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('2D PCA View', '3D PCA View'),\n",
    "        specs=[[{'type': 'scatter'}, {'type': 'scatter3d'}]]\n",
    "    )\n",
    "    \n",
    "    # 2D visualization\n",
    "    colors = px.colors.qualitative.Set3[:len(np.unique(labels))]\n",
    "    \n",
    "    for i, cluster in enumerate(np.unique(labels)):\n",
    "        mask = labels == cluster\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_pca[mask, 0],\n",
    "                y=X_pca[mask, 1],\n",
    "                mode='markers',\n",
    "                name=f'Cluster {cluster}',\n",
    "                marker=dict(color=colors[i], size=4, opacity=0.7)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # 3D visualization\n",
    "    if X_pca.shape[1] >= 3:\n",
    "        for i, cluster in enumerate(np.unique(labels)):\n",
    "            mask = labels == cluster\n",
    "            fig.add_trace(\n",
    "                go.Scatter3d(\n",
    "                    x=X_pca[mask, 0],\n",
    "                    y=X_pca[mask, 1],\n",
    "                    z=X_pca[mask, 2],\n",
    "                    mode='markers',\n",
    "                    name=f'Cluster {cluster}',\n",
    "                    marker=dict(color=colors[i], size=3, opacity=0.6),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Create visualizations\n",
    "visualize_clusters(X_pca, final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and preprocessing pipeline\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "model_dir = '../models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save components\n",
    "joblib.dump(pca, f'{model_dir}/pca_transformer.pkl')\n",
    "joblib.dump(best_kmeans, f'{model_dir}/kmeans_model.pkl')\n",
    "joblib.dump(encoders, f'{model_dir}/encoders.pkl')\n",
    "\n",
    "# Save model metadata\n",
    "model_info = {\n",
    "    'silhouette_score': final_score,\n",
    "    'n_clusters': best_k,\n",
    "    'n_pca_components': n_components,\n",
    "    'feature_columns': feature_cols,\n",
    "    'target_achieved': final_score > 0.85\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{model_dir}/model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"✅ Model and preprocessing pipeline saved successfully!\")\n",
    "print(f\"📁 Saved to: {model_dir}\")\n",
    "print(f\"🎯 Final Silhouette Score: {final_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
